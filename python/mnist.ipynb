{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized SNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Other\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_path  = './data/mnist'\n",
    "num_class  = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0,), (1,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Make a sanity check plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for i in range(3):\n",
    "    image, label = mnist_train[i]\n",
    "    axs[i].imshow(image.squeeze(0).numpy(), cmap=\"gray\")\n",
    "    axs[i].set_title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataloader and Spike Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Fetch a batch of data\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "# Encode it into spikes\n",
    "#   - linear, normalize, clip are IMPORTANT for the spikegen.latency function\n",
    "#     details here: https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_1.html\n",
    "spike_data = spikegen.latency(data_it, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "\n",
    "# Plot to keep me sane\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(12, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "splt.raster(spike_data[:, 0].view(100, -1), ax, s=25, c=\"black\")\n",
    "plt.title(\"Input Layer\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Neuron Number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(32*32, 10),\n",
    "    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data):\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(net)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        x = data[step]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        spk_out, mem_out = net(x)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "\n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "spike_data = spikegen.latency(data_it, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "spike_data = spike_data.to(device)\n",
    "spk_rec, mem_rec = forward_pass(net, 100, spike_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "acc_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "num_epochs = 1\n",
    "num_iters = 50\n",
    "num_steps = 100\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        data = spikegen.latency(data, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(acc_hist)\n",
    "plt.title(\"Train Set Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
